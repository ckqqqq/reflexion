{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to set scratchpad and reflections\n",
    "import re\n",
    "import string\n",
    "from react_cls import parse_action, ReactReflectAgent\n",
    "from util import summarize_trial, log_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/reflect/100_questions_5_trials.txt', 'r') as f:\n",
    "    log = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = []\n",
    "last_trial = re.split('#+\\nBEGIN TRIAL \\d+\\nTrial summary: Correct: \\d+, Incorrect: \\d+, Halted: \\d+\\n#+', log)[-1]\n",
    "scratchpads = [s for s in last_trial.split('\\n\\n') if s.startswith('Question')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sp in scratchpads:\n",
    "    lines = sp.split('\\n')\n",
    "\n",
    "    # Get question\n",
    "    question = lines[0].strip('Question: ').strip()\n",
    "\n",
    "    # Get reflections\n",
    "    reflections = [s.strip('- ') for s in lines if s.startswith('-')]\n",
    "\n",
    "    # Get key\n",
    "    key_line = [s for s in lines if s.startswith('Correct')][0]\n",
    "    key_ind = lines.index(key_line)\n",
    "    key = key_line.split('Correct answer: ')[1]\n",
    "\n",
    "    # Get answer\n",
    "    answer = ''\n",
    "    finish = [parse_action(': '.join(s.split(': ')[1:]))[1] for s in lines if 'Finish' in s and s.startswith('Action')]\n",
    "    if len(finish) != 0:\n",
    "        answer = finish[0].strip()\n",
    "\n",
    "    agent = ReactReflectAgent(question, key)\n",
    "    agent.answer = answer\n",
    "    agent.reflections = reflections\n",
    "    agents.append(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = 5\n",
    "log = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 0\n",
    "agents_to_run = [a for a in agents if not a.is_correct()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 6 (0/46)\n",
      "Trial: 6 (1/46)\n",
      "Trial: 6 (2/46)\n",
      "Trial: 6 (3/46)\n",
      "Trial: 6 (4/46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/becklabash/Desktop/Projects/hedilog/hedilog-agents-with-memory/env/lib/python3.8/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/becklabash/Desktop/Projects/hedilog/hedilog-agents-with-memory/env/lib/python3.8/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 6 (5/46)\n",
      "Trial: 6 (6/46)\n",
      "Trial: 6 (7/46)\n",
      "Trial: 6 (8/46)\n",
      "Trial: 6 (9/46)\n",
      "Trial: 6 (10/46)\n",
      "Trial: 6 (11/46)\n",
      "Trial: 6 (12/46)\n",
      "Trial: 6 (13/46)\n",
      "Trial: 6 (14/46)\n",
      "Trial: 6 (15/46)\n",
      "Trial: 6 (16/46)\n",
      "Trial: 6 (17/46)\n",
      "Trial: 6 (18/46)\n",
      "'fullurl'\n",
      "'fullurl'\n",
      "'fullurl'\n",
      "'fullurl'\n",
      "'fullurl'\n",
      "Trial: 6 (19/46)\n",
      "Trial: 6 (20/46)\n",
      "Trial: 6 (21/46)\n",
      "Trial: 6 (22/46)\n",
      "Trial: 6 (23/46)\n",
      "Trial: 6 (24/46)\n",
      "Trial: 6 (25/46)\n",
      "Trial: 6 (26/46)\n",
      "Trial: 6 (27/46)\n",
      "Trial: 6 (28/46)\n",
      "Trial: 6 (29/46)\n",
      "Trial: 6 (30/46)\n",
      "Trial: 6 (31/46)\n",
      "Trial: 6 (32/46)\n",
      "Trial: 6 (33/46)\n",
      "Trial: 6 (34/46)\n",
      "Trial: 6 (35/46)\n",
      "Trial: 6 (36/46)\n",
      "Trial: 6 (37/46)\n",
      "Trial: 6 (38/46)\n",
      "Trial: 6 (39/46)\n",
      "'fullurl'\n",
      "'fullurl'\n",
      "'fullurl'\n",
      "'fullurl'\n",
      "'fullurl'\n",
      "Trial: 6 (40/46)\n",
      "Trial: 6 (41/46)\n",
      "Trial: 6 (42/46)\n",
      "Trial: 6 (43/46)\n",
      "Trial: 6 (44/46)\n",
      "Trial: 6 (45/46)\n",
      "'fullurl'\n",
      "'fullurl'\n",
      "'fullurl'\n",
      "'fullurl'\n",
      "'fullurl'\n",
      "Finished Trial 7, Correct: 53, Incorrect: 37, Halted: 9\n"
     ]
    }
   ],
   "source": [
    "while q < len(agents_to_run):\n",
    "    print(f'Trial: {trial} ({q}/{len(agents_to_run)})')\n",
    "    try:\n",
    "        agents_to_run[q].run()\n",
    "    except:\n",
    "        pass\n",
    "    q += 1\n",
    "\n",
    "trial += 1\n",
    "\n",
    "log += log_trial(agents, trial)\n",
    "correct, incorrect, halted = summarize_trial(agents)\n",
    "print(f'Finished Trial {trial}, Correct: {len(correct)}, Incorrect: {len(incorrect)}, Halted: {len(halted)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/reflect/100_questions_trials_6_to_7.txt', 'w') as f:\n",
    "    f.write(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from prompts import REFLECTION_HEADER\n",
    "\n",
    "def format_reflections(reflections: List[str]) -> str:\n",
    "    if reflections == []:\n",
    "        return ''\n",
    "    else:\n",
    "        header = REFLECTION_HEADER\n",
    "        return header + 'Reflections:\\n- ' + '\\n- '.join([r.strip() for r in reflections])\n",
    "    \n",
    "def summarize_trial(agents):\n",
    "    correct = [a for a in agents if a.is_correct()]\n",
    "    halted = [a for a in agents if a.is_halted()]\n",
    "    incorrect = [a for a in agents if a.is_finished() and not a.is_correct()]\n",
    "    return correct, incorrect, halted\n",
    "\n",
    "    # (Not correct) and (not halted) and (not finished and correct)\n",
    "def log_trial(agents: List, trial_n):\n",
    "    correct, incorrect, halted = summarize_trial(agents)\n",
    "\n",
    "    log = f\"\"\"\n",
    "########################################\n",
    "BEGIN TRIAL {trial_n}\n",
    "Trial summary: Correct: {len(correct)}, Incorrect: {len(incorrect)}, Halted: {len(halted)}\n",
    "#######################################\n",
    "\"\"\"\n",
    "\n",
    "    log += '------------- BEGIN CORRECT AGENTS -------------\\n\\n'\n",
    "    for agent in correct:\n",
    "        log += f'Question: {agent.question}{format_reflections(agent.reflections)}{agent.scratchpad}\\nCorrect answer: {agent.key}\\n\\n'\n",
    "\n",
    "    log += '------------- BEGIN INCORRECT AGENTS -----------\\n\\n'\n",
    "    for agent in incorrect:\n",
    "        log += f'Question: {agent.question}{format_reflections(agent.reflections)}{agent.scratchpad}\\nCorrect answer: {agent.key}\\n\\n'\n",
    "\n",
    "    log += '------------- BEGIN HALTED AGENTS --------------\\n\\n'\n",
    "    for agent in halted:\n",
    "        log += f'Question: {agent.question}{format_reflections(agent.reflections)}{agent.scratchpad}\\nCorrect answer: {agent.key}\\n\\n'\n",
    "\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/reflect/100_questions_5_trials.txt', 'r') as f:\n",
    "    one_to_five = f.read()\n",
    "\n",
    "with open('output/reflect/100_questions_trials_6_to_7.txt', 'r') as f:\n",
    "    six_to_seven = f.read()\n",
    "\n",
    "one_to_seven = one_to_five + '\\n\\n' + six_to_seven\n",
    "\n",
    "with open('output/reflect/100_questions_5_trials.txt', 'w') as f:\n",
    "    f.write(one_to_seven)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
